````md
# Create and Modified ISO for Adding New Agent-Based Nodes to OpenShift

This document describes a workflow to create a Day 2 (agent-based) ISO and then **customize** it using `coreos-installer iso customize`.  
Goal: boot a new node, fetch its Ignition remotely, and install OpenShift cleanly on the target disk. 

## What you get at the end

- A standard Day 2 ISO generated by `oc adm node-image create`
- A customized ISO that:
  - boots with useful debug + serial console settings
  - loads the live environment into RAM (reduces disk conflicts and “device busy” situations)
  - installs to a defined device
  - pulls Ignition from your webserver URL

---

## Prerequisites

- You have `oc` access to the cluster as a user that can read the Machine Config Operator secrets.
- `coreos-installer` is installed on the system where you build the ISO.
- Your environment can serve Ignition over HTTP (simple webserver is enough).
- You know the target install device (example: `/dev/sda`).

---

## 1. Prepare a working directory

This block sets up a clean workspace and points tooling to your pull secret.

```bash
export REGISTRY_AUTH_FILE=/home/opc/PullSecret.json
export DIR=day2
mkdir -p $DIR
cd $DIR
````

Notes:

* `REGISTRY_AUTH_FILE` is used by OpenShift tooling to authenticate against registries when needed.
* Keeping everything in a dedicated directory makes it easier to archive the build inputs and outputs.

---

## 2. Export the worker Ignition from the cluster

This block extracts the worker Ignition from the cluster. This is common for **UPI / External Platform** setups where you manage nodes outside of Machine API.

```bash
oc -n openshift-machine-config-operator get secret worker-user-data \
  -o jsonpath='{.data.userData}' | base64 -d > worker.ign
```

What happens:

* `worker-user-data` is stored as base64 in the secret.
* The command decodes it into `worker.ign`, which is the Ignition config the new node needs.

---

## 3. Publish Ignition on a webserver

The customized ISO will download Ignition from a URL. This block places the file into a web root with safe permissions.

```bash
sudo mkdir -p /var/www/html/day2
sudo cp worker.ign /var/www/html/day2/worker.ign
sudo chmod 0644 /var/www/html/day2/worker.ign
```

Notes:

* `0644` is typically fine for Ignition delivery over HTTP in controlled environments.
* Make sure your firewall / security lists allow the new node to reach this webserver.

---

## 4. Validate Ignition is reachable and valid JSON

This block checks two things:

1. the webserver actually serves the file
2. the content is valid JSON

```bash
curl -sS http://webserver/day2/worker.ign | head -n 2
curl -sS http://webserver/day2/worker.ign | jq -e . >/dev/null && echo OK
```

Expected result:

* The first command prints the first lines (quick sanity check).
* The second prints `OK` if the JSON parses cleanly.

---

## 5. Create the default Day 2 ISO

This is the baseline ISO creation using OpenShift tooling.

```bash
oc adm node-image create \
  --mac-address=00:00:00:00:00:00 \
  --root-device-hint=deviceName:/dev/sda
```

Notes:

* `--mac-address` is required by the command. If you generate per-host ISOs you should set the real MAC.
* `--root-device-hint` helps the installer select the intended disk (important on systems with multiple devices).

Output:

* The command creates a `node.x86_64.iso` (as referenced in the next steps).

---

## 6. Customize the Day 2 ISO (coreos-installer)

This is the supported customization path using `coreos-installer iso customize`.
It injects kernel arguments (kargs) into the ISO so you don’t need manual intervention during boot.

```bash
coreos-installer iso customize -f \
  --live-karg-append console=tty0 \
  --live-karg-append console=ttyS0,115200n8 \
  --live-karg-append rd.debug \
  --live-karg-append rd.live.ram=1 \
  --live-karg-append rd.live.overlay=none \
  --live-karg-append rd.luks=0 \
  --live-karg-append rd.lvm=0 \
  --live-karg-append rd.md=0 \
  --live-karg-append rd.dm=0 \
  --live-karg-append coreos.liveiso.fromram \
  --live-karg-append coreos.inst.install_dev=/dev/sda \
  --live-karg-append coreos.inst.ignition_url=http://webserver/day2/worker.ign \
  -o rhcos-auto-worker-oci-ram-debug-DAY2-1.iso \
node.x86_64.iso
```

What the important kargs do:

* `console=tty0` and `console=ttyS0,115200n8`: local + serial console output (useful on VMs and bare metal).
* `rd.debug`: verbose initramfs debug output (helpful when troubleshooting early boot).
* `rd.live.ram=1` and `coreos.liveiso.fromram`: run the live environment from RAM (reduces disk contention and makes installs more deterministic).
* `rd.live.overlay=none`: disables overlay, keeps behavior simpler and more predictable during troubleshooting.
* `rd.luks=0`, `rd.lvm=0`, `rd.md=0`, `rd.dm=0`: disables scanning/activation of encryption, LVM, mdraid, device-mapper. This can prevent unexpected device grabs in mixed environments.
* `coreos.inst.install_dev=/dev/sda`: forces the install target.
* `coreos.inst.ignition_url=...`: fetches Ignition from your webserver at install time / first boot path.

---

## 7. Verify the ISO customization

This block prints the embedded kernel arguments from the ISO so you can confirm the final artifact is correct.

```bash
coreos-installer iso kargs show rhcos-auto-worker-oci-ram-debug-DAY2-1.iso
```

Expected result:

* You should see all appended kargs listed, including the `coreos.inst.ignition_url` and `coreos.inst.install_dev`.

---

## 8. Transfer ISO and launch the node

At this point the ISO is ready.

Typical options:

* Upload to an object storage bucket and boot from it
* Attach/mount in console
* Use SDK or API driven provisioning
* Use an existing stack or pipeline you already have

Key requirement:

* The node must be able to reach `http://webserver/day2/worker.ign` during install/boot.

---

## 9. Troubleshooting on the new node after boot

These commands help confirm that:

* the node booted with the intended kernel args
* coreos-installer and ignition ran and produced logs

```bash
cat /proc/cmdline
journalctl -b -u coreos-installer -u ignition-*
```

What to look for:

* `/proc/cmdline` contains your appended arguments.
* `coreos-installer` logs show the target disk and install progress.
* `ignition-*` logs show Ignition fetch, validation, and application.

---

## 10. Monitor node creation and join process from the cluster side

This monitors a node by IP during provisioning and CSR flow.

```bash
oc adm node-image monitor --ip-addresses <node IP address>
```

Example output (for reference):

```text
2026-02-12T09:21:48Z [node-image monitor] installer pullspec obtained from installer-images configMap quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:93432b35291e54c4871476e4c4d941f25a050436319e27dd8f01c8560ce7b5b0
2026-02-12T09:21:48Z [node-image monitor] Launching command
2026-02-12T09:21:53Z [node-image monitor] Monitoring IPs: <node IP address>
2026-02-12T09:23:04Z [node-image monitor] Node 10.0.1.38: Kubelet is running
2026-02-12T09:23:04Z [node-image monitor] Node 10.0.1.38: First CSR Pending approval
2026-02-12T09:23:04Z [node-image monitor] Node 10.0.1.38: CSR csr-sthdd with signerName kubernetes.io/kube-apiserver-client-kubelet and username system:serviceaccount:openshift-machine-config-operator:node-bootstrapper is Pending and awaiting approval
2026-02-12T09:23:59Z [node-image monitor] Node 10.0.1.38: Second CSR Pending approval
2026-02-12T09:23:59Z [node-image monitor] Node 10.0.1.38: CSR csr-7mwp2 with signerName kubernetes.io/kubelet-serving and username system:node:rhcos-test02.clu.<your-domain>.com is Pending and awaiting approval
2026-02-12T09:23:59Z [node-image monitor] Node 10.0.1.38: Node joined cluster
2026-02-12T09:25:14Z [node-image monitor] Node 10.0.1.38: Node is Ready
```

Notes:

* The monitor output is especially useful to see where you are stuck: Ignition, kubelet, CSR approval, or final readiness.

---

## Common pitfalls

* Wrong install device: `coreos.inst.install_dev=/dev/sda` must match reality.
* Ignition not reachable: routing, firewall, DNS, or wrong URL.
* CSR approval not automated: node join can stall waiting for approval (depends on your cluster policy).
* Console visibility: on OCI or bare metal, `console=ttyS0` is often the difference between guessing and knowing.

---

